{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba40eab2-b970-4bf7-bb6e-f03a8a77f723",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>id_str</th>\n",
       "      <th>image_url</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1814196917410357309</td>\n",
       "      <td>Fri Jul 19 07:26:50 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@tanyarlfes org cabul mah cabul ga liat ras ajg</td>\n",
       "      <td>1814200206340358196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tanyarlfes</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/calypsoore/status/18142002063403...</td>\n",
       "      <td>1540746774364557317</td>\n",
       "      <td>calypsoore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1814197629259264509</td>\n",
       "      <td>Fri Jul 19 07:26:50 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@fachrianantyo 200k katanya</td>\n",
       "      <td>1814200206105428467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fachrianantyo</td>\n",
       "      <td>in</td>\n",
       "      <td>Tangerang, Indonesia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/malvinico/status/181420020610542...</td>\n",
       "      <td>90125063</td>\n",
       "      <td>malvinico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1814200205937418468</td>\n",
       "      <td>Fri Jul 19 07:26:49 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>PERP H4RRY</td>\n",
       "      <td>1814200205937418468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/svtn28/status/1814200205937418468</td>\n",
       "      <td>1411459219954540545</td>\n",
       "      <td>svtn28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1813795378116063597</td>\n",
       "      <td>Fri Jul 19 07:26:49 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@cocoro_rods @jawafess Bosku orang situ asli d...</td>\n",
       "      <td>1814200205702832341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cocoro_rods</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/hellow__me/status/18142002057028...</td>\n",
       "      <td>1307649603303473152</td>\n",
       "      <td>hellow__me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1814087866806624355</td>\n",
       "      <td>Fri Jul 19 07:26:49 +0000 2024</td>\n",
       "      <td>0</td>\n",
       "      <td>@masgah_ Pernah pas sd</td>\n",
       "      <td>1814200205614743990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>masgah_</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://x.com/tekananturgor/status/18142002056...</td>\n",
       "      <td>1272476969079201794</td>\n",
       "      <td>tekananturgor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   conversation_id_str                      created_at  favorite_count  \\\n",
       "0  1814196917410357309  Fri Jul 19 07:26:50 +0000 2024               0   \n",
       "1  1814197629259264509  Fri Jul 19 07:26:50 +0000 2024               0   \n",
       "2  1814200205937418468  Fri Jul 19 07:26:49 +0000 2024               0   \n",
       "3  1813795378116063597  Fri Jul 19 07:26:49 +0000 2024               0   \n",
       "4  1814087866806624355  Fri Jul 19 07:26:49 +0000 2024               0   \n",
       "\n",
       "                                           full_text               id_str  \\\n",
       "0    @tanyarlfes org cabul mah cabul ga liat ras ajg  1814200206340358196   \n",
       "1                        @fachrianantyo 200k katanya  1814200206105428467   \n",
       "2                                         PERP H4RRY  1814200205937418468   \n",
       "3  @cocoro_rods @jawafess Bosku orang situ asli d...  1814200205702832341   \n",
       "4                             @masgah_ Pernah pas sd  1814200205614743990   \n",
       "\n",
       "  image_url in_reply_to_screen_name lang              location  quote_count  \\\n",
       "0       NaN              tanyarlfes   in                   NaN            0   \n",
       "1       NaN           fachrianantyo   in  Tangerang, Indonesia            0   \n",
       "2       NaN                     NaN   in                   NaN            0   \n",
       "3       NaN             cocoro_rods   in                   NaN            0   \n",
       "4       NaN                 masgah_   in                   NaN            0   \n",
       "\n",
       "   reply_count  retweet_count  \\\n",
       "0            0              0   \n",
       "1            0              0   \n",
       "2            0              0   \n",
       "3            0              0   \n",
       "4            0              0   \n",
       "\n",
       "                                           tweet_url          user_id_str  \\\n",
       "0  https://x.com/calypsoore/status/18142002063403...  1540746774364557317   \n",
       "1  https://x.com/malvinico/status/181420020610542...             90125063   \n",
       "2    https://x.com/svtn28/status/1814200205937418468  1411459219954540545   \n",
       "3  https://x.com/hellow__me/status/18142002057028...  1307649603303473152   \n",
       "4  https://x.com/tekananturgor/status/18142002056...  1272476969079201794   \n",
       "\n",
       "        username  \n",
       "0     calypsoore  \n",
       "1      malvinico  \n",
       "2         svtn28  \n",
       "3     hellow__me  \n",
       "4  tekananturgor  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "TWEET_DATA = pd.read_csv(\"crawling-data/dataset_sementara.csv\")\n",
    "\n",
    "TWEET_DATA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34510d56-3016-47de-8812-5dcf9700dcc6",
   "metadata": {},
   "source": [
    "#### Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb8852c-1088-422f-9481-80238ddd40b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding Result : \n",
      "\n",
      "0      @tanyarlfes org cabul mah cabul ga liat ras ajg\n",
      "1                          @fachrianantyo 200k katanya\n",
      "2                                           perp h4rry\n",
      "3    @cocoro_rods @jawafess bosku orang situ asli d...\n",
      "4                               @masgah_ pernah pas sd\n",
      "Name: full_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# ------ Case Folding --------\n",
    "# gunakan fungsi Series.str.lower() pada Pandas\n",
    "TWEET_DATA['full_text'] = TWEET_DATA['full_text'].str.lower()\n",
    "\n",
    "\n",
    "print('Case Folding Result : \\n')\n",
    "print(TWEET_DATA['full_text'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4455a8-3024-40c6-bfff-22b6a8e5bb2f",
   "metadata": {},
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afcb6273-6f83-46ba-8cc6-fa5a8cac23fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Result : \n",
      "\n",
      "0         [org, cabul, mah, cabul, ga, liat, ras, ajg]\n",
      "1                                            [katanya]\n",
      "2                                         [perp, hrry]\n",
      "3    [rods, bosku, orang, situ, asli, dan, bukan, p...\n",
      "4                                    [pernah, pas, sd]\n",
      "Name: tweet_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "import re #regex library\n",
    "\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# ------ Tokenizing ---------\n",
    "\n",
    "def remove_tweet_special(text):\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    # remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "                \n",
    "TWEET_DATA['full_text'] = TWEET_DATA['full_text'].apply(remove_tweet_special)\n",
    "\n",
    "#remove number\n",
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "TWEET_DATA['full_text'] = TWEET_DATA['full_text'].apply(remove_number)\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "TWEET_DATA['full_text'] = TWEET_DATA['full_text'].apply(remove_punctuation)\n",
    "\n",
    "#remove whitespace leading & trailing\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "TWEET_DATA['full_text'] = TWEET_DATA['full_text'].apply(remove_whitespace_LT)\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "TWEET_DATA['full_text'] = TWEET_DATA['full_text'].apply(remove_whitespace_multiple)\n",
    "\n",
    "# remove single char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "TWEET_DATA['full_text'] = TWEET_DATA['full_text'].apply(remove_singl_char)\n",
    "\n",
    "# NLTK word rokenize \n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "TWEET_DATA['tweet_tokens'] = TWEET_DATA['full_text'].apply(word_tokenize_wrapper)\n",
    "\n",
    "print('Tokenizing Result : \\n') \n",
    "print(TWEET_DATA['tweet_tokens'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c087e35c-8d7d-4ff6-a85b-563e2fd2a413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Tokens : \n",
      "\n",
      "0    [(cabul, 2), (org, 1), (mah, 1), (ga, 1), (lia...\n",
      "1                                       [(katanya, 1)]\n",
      "2                               [(perp, 1), (hrry, 1)]\n",
      "3    [(rods, 1), (bosku, 1), (orang, 1), (situ, 1),...\n",
      "4                     [(pernah, 1), (pas, 1), (sd, 1)]\n",
      "Name: tweet_tokens_fdist, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# NLTK calc frequency distribution\n",
    "def freqDist_wrapper(text):\n",
    "    return FreqDist(text)\n",
    "\n",
    "TWEET_DATA['tweet_tokens_fdist'] = TWEET_DATA['tweet_tokens'].apply(freqDist_wrapper)\n",
    "\n",
    "print('Frequency Tokens : \\n') \n",
    "print(TWEET_DATA['tweet_tokens_fdist'].head().apply(lambda x : x.most_common()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c277f41-50c3-42ce-a04c-78d4dbf9e036",
   "metadata": {},
   "source": [
    "#### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbfc5df9-f070-4027-9d33-e26bdff6b3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             [org, cabul, mah, cabul, liat, ras, ajg]\n",
      "1                                                   []\n",
      "2                                         [perp, hrry]\n",
      "3    [rods, bosku, orang, situ, asli, pendatang, be...\n",
      "4                                            [pas, sd]\n",
      "Name: tweet_tokens_WSW, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "# ---------------------------- manualy add stopword  ------------------------------------\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah'])\n",
    "\n",
    "# ----------------------- add stopword from txt file ------------------------------------\n",
    "# read txt stopword using pandas\n",
    "# txt_stopword = pd.read_csv(\"stopwords.txt\", names= [\"stopwords\"], header = None)\n",
    "\n",
    "# # convert stopword string to list & append additional stopword\n",
    "# list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "TWEET_DATA['tweet_tokens_WSW'] = TWEET_DATA['tweet_tokens'].apply(stopwords_removal) \n",
    "\n",
    "\n",
    "print(TWEET_DATA['tweet_tokens_WSW'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470b403b-94d8-4b8a-9857-9b7254e4e82c",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5b06fde-d0b4-47ab-be44-d52a1e6d37c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           [orang, cabul, mah, cabul, liat, ras, ajg]\n",
       "1                                                   []\n",
       "2                                         [perp, hrry]\n",
       "3    [rods, bosku, orang, situ, asli, pendatang, be...\n",
       "4                                            [pas, sd]\n",
       "5             [waspada, paham, radikalisme, terorisme]\n",
       "6                            [nontonnya, dimana, link]\n",
       "7    [info, akun, like, percakapan, bokep, porno, l...\n",
       "8                     [kirain, pisang, dibelah, laper]\n",
       "9                            [ditunggutunggu, coyyyyy]\n",
       "Name: tweet_normalized, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizad_word = pd.read_csv(\"normalisasi.csv\")\n",
    "\n",
    "normalizad_word_dict = {}\n",
    "\n",
    "for index, row in normalizad_word.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1] \n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "TWEET_DATA['tweet_normalized'] = TWEET_DATA['tweet_tokens_WSW'].apply(normalized_term)\n",
    "\n",
    "TWEET_DATA['tweet_normalized'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4b746-68a0-44c0-aa9a-d583aaeea664",
   "metadata": {},
   "source": [
    "#### Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d98dee-87c2-4d18-8fca-ed1c2afb67ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "893\n",
      "------------------------\n",
      "orang : orang\n",
      "cabul : cabul\n",
      "mah : mah\n",
      "liat : liat\n",
      "ras : ras\n",
      "ajg : ajg\n",
      "perp : perp\n",
      "hrry : hrry\n",
      "rods : rods\n",
      "bosku : bos\n",
      "situ : situ\n",
      "asli : asli\n",
      "pendatang : datang\n",
      "beraniberani : beraniberani\n",
      "pas : pas\n",
      "sd : sd\n",
      "waspada : waspada\n",
      "paham : paham\n",
      "radikalisme : radikalisme\n",
      "terorisme : terorisme\n",
      "nontonnya : nontonnya\n",
      "dimana : mana\n",
      "link : link\n",
      "info : info\n",
      "akun : akun\n",
      "like : like\n",
      "percakapan : cakap\n",
      "bokep : bokep\n",
      "porno : porno\n",
      "lokal : lokal\n",
      "full : full\n",
      "durasi : durasi\n",
      "kirain : kirain\n",
      "pisang : pisang\n",
      "dibelah : belah\n",
      "laper : laper\n",
      "ditunggutunggu : ditunggutunggu\n",
      "coyyyyy : coyyyyy\n",
      "hyemin : hyemin\n",
      "alhamdulillah : alhamdulillah\n",
      "uktnya : uktnya\n",
      "tingkat : tingkat\n",
      "tetapi : tetapi\n",
      "juta : juta\n",
      "hosqyvhuabvfpmptzfewsqonmxpmhhuiqcti : hosqyvhuabvfpmptzfewsqonmxpmhhuiqcti\n",
      "anakku : anak\n",
      "nangis : nang\n",
      "pulang : pulang\n",
      "betah : betah\n",
      "banget : banget\n",
      "sekolah : sekolah\n",
      "agakakao : agakakao\n",
      "pacet : pacet\n",
      "pls : pls\n",
      "chudai : chudai\n",
      "pporno : pporno\n",
      "christmas : christmas\n",
      "na : na\n",
      "eid : eid\n",
      "zitakuwa : zitakuwa\n",
      "siku : siku\n",
      "moja : moja\n",
      "tutabadilishana : tutabadilishana\n",
      "vyakula : vyakula\n",
      "tu : tu\n",
      "hiyo : hiyo\n",
      "upaya : upaya\n",
      "tindak : tindak\n",
      "aparat : aparat\n",
      "keamanan : aman\n",
      "opm : opm\n",
      "kelompok : kelompok\n",
      "teranus : anus\n",
      "enumbi : enumbi\n",
      "daeng : daeng\n",
      "gelar : gelar\n",
      "kakak : kakak\n",
      "gw : gw\n",
      "jogging : jogging\n",
      "jajan : jajan\n",
      "mari : mari\n",
      "bangkit : bangkit\n",
      "wkwkwk : wkwkwk\n",
      "begitu : begitu\n",
      "semangatinnya : semangatinnya\n",
      "koi : koi\n",
      "shaq : shaq\n",
      "nai : nai\n",
      "ha : ha\n",
      "is : is\n",
      "ma : ma\n",
      "makam : makam\n",
      "gusdur : gusdur\n",
      "lihat : lihat\n",
      "pondok : pondok\n",
      "suami : suami\n",
      "jaman : jaman\n",
      "red : red\n",
      "flag : flag\n",
      "jt : jt\n",
      "an : an\n",
      "salah : salah\n",
      "yaakk : yaakk\n",
      "fcuk : fcuk\n",
      "mera : mera\n",
      "bhi : bhi\n",
      "yahi : yahi\n",
      "issue : issue\n",
      "aya : aya\n",
      "panass : panass\n",
      "tidak apa apa : tidak apa apa\n",
      "bagus : bagus\n",
      "drpd : drpd\n",
      "dtg : dtg\n",
      "saya : saya\n",
      "tkut : tkut\n",
      "wkwk : wkwk\n",
      "colets : colets\n",
      "aiah : aiah\n",
      "ko : ko\n",
      "jawa : jawa\n",
      "nasi : nasi\n",
      "bihun : bihun\n",
      "goreng : goreng\n",
      "urap : urap\n",
      "tempe : tempe\n",
      "rp : rp\n",
      "jumat : jumat\n",
      "berkah : berkah\n",
      "lauk : lauk\n",
      "kisaran : kisar\n",
      "rbrb : rbrb\n",
      "diam : diam\n",
      "lawan : lawan\n",
      "allah : allah\n",
      "mampus : mampus\n",
      "kau : kau\n",
      "ler : ler\n",
      "ba : ba\n",
      "mutualan : mutualan\n",
      "yuk : yuk\n",
      "suka : suka\n",
      "ka : ka\n",
      "sunghoon : sunghoon\n",
      "berani : berani\n",
      "jujur : jujur\n",
      "direvisi : revisi\n",
      "spy : spy\n",
      "sesuai : sesuai\n",
      "etika : etika\n",
      "kekinian : kini\n",
      "emg : emg\n",
      "abad : abad\n",
      "terus : terus\n",
      "bagaimana : bagaimana\n",
      "ngantuk : ngantuk\n",
      "bangettt : bangettt\n",
      "kak : kak\n",
      "kamuu : kamuu\n",
      "ngisi : ngisi\n",
      "tauu : tauu\n",
      "angoota : angoota\n",
      "katwa : katwa\n",
      "dei : dei\n",
      "guru : guru\n",
      "ji : ji\n",
      "kerasa : rasa\n",
      "kontraknya : kontrak\n",
      "sudah : sudah\n",
      "abis : abis\n",
      "burem : burem\n",
      "revisi : revisi\n",
      "perasaan : asa\n",
      "kmrn : kmrn\n",
      "bunyi : bunyi\n",
      "tergantung : gantung\n",
      "dosen : dosen\n",
      "kali : kali\n",
      "nanyain : nanyain\n",
      "ditanyain : ditanyain\n",
      "psti : psti\n",
      "kesepakatan : sepakat\n",
      "dosennya : dosen\n",
      "most : most\n",
      "likely : likely\n",
      "see : see\n",
      "bandung : bandung\n",
      "aman : aman\n",
      "habis : habis\n",
      "yt : yt\n",
      "taemin : taemin\n",
      "oktober : oktober\n",
      "kesini : kesini\n",
      "agustus : agustus\n",
      "tiketing : tiketing\n",
      "yaallah : yaallah\n",
      "bayar : bayar\n",
      "semesteran : semester\n",
      "done : done\n",
      "wml : wml\n",
      "semoga : moga\n",
      "album : album\n",
      "smoothie : smoothie\n",
      "era : era\n",
      "hshshs : hshshs\n",
      "anw : anw\n",
      "ayo : ayo\n",
      "kakk : kakk\n",
      "walikum : walikum\n",
      "asslam : asslam\n",
      "dear : dear\n",
      "khair : khair\n",
      "mubarak : mubarak\n",
      "khush : khush\n",
      "rhein : rhein\n",
      "saking : saking\n",
      "capeknya : capek\n",
      "ketawa : ketawa\n",
      "pipinya : pipi\n",
      "kecapekan : cape\n",
      "join : join\n",
      "reseller : reseller\n",
      "diajarin : diajarin\n",
      "sampe : sampe\n",
      "gillzzz : gillzzz\n",
      "bgtt : bgtt\n",
      "loee : loee\n",
      "ment : ment\n",
      "aduh : aduh\n",
      "kemesraan : mesra\n",
      "malam : malam\n",
      "terganggu : ganggu\n",
      "kokomi : kokomi\n",
      "so : so\n",
      "cinnamoroll : cinnamoroll\n",
      "coded : coded\n",
      "langsung : langsung\n",
      "nonton : nonton\n",
      "maraton : maraton\n",
      "bertebaran : tebar\n",
      "spoiler : spoiler\n",
      "tiktok : tiktok\n",
      "ayaamm : ayaamm\n",
      "geprek : geprek\n",
      "emang : emang\n",
      "terbaikkk : terbaikkk\n",
      "selamat : selamat\n",
      "makaan : maka\n",
      "charisaa : charisaa\n",
      "barang : barang\n",
      "ngopi : ngopi\n",
      "bareng : bareng\n",
      "viral : viral\n",
      "terbaru : baru\n",
      "bocil : bocil\n",
      "sampai : sampai\n",
      "dipaksa : paksa\n",
      "ayah : ayah\n",
      "tiri : tiri\n",
      "ngn : ngn\n",
      "india : india\n",
      "doodstream : doodstream\n",
      "live : live\n",
      "ojol : ojol\n",
      "abg : abg\n",
      "mango : mango\n",
      "hijab : hijab\n",
      "jilbob : jilbob\n",
      "jilbab : jilbab\n",
      "seleb : seleb\n",
      "indonesia : indonesia\n",
      "prank : prank\n",
      "cewe : cewe\n",
      "hamil : hamil\n",
      "cantik : cantik\n",
      "ometv : ometv\n",
      "bling : bling\n",
      "tobrut : tobrut\n",
      "minggu : minggu\n",
      "linkk : linkk\n",
      "iti : iti\n",
      "mpataka : mpataka\n",
      "manusia : manusia\n",
      "kuat : kuat\n",
      "mp : mp\n",
      "tahan : tahan\n",
      "rud : rud\n",
      "jam : jam\n",
      "bhasha : bhasha\n",
      "korean : korean\n",
      "version : version\n",
      "muh : muh\n",
      "le : le\n",
      "gora : gora\n",
      "kr : kr\n",
      "do : do\n",
      "hadiah : hadiah\n",
      "masuk : masuk\n",
      "kuliah : kuliah\n",
      "papa : papa\n",
      "genryusai : genryusai\n",
      "oh : oh\n",
      "daster : daster\n",
      "merah : merah\n",
      "koleksi : koleksi\n",
      "rara : rara\n",
      "nadifa : nadifa\n",
      "main : main\n",
      "enak : enak\n",
      "sempit : sempit\n",
      "sma : sma\n",
      "anime : anime\n",
      "tante : tante\n",
      "aaaaaaaaa : aaaaaaaaa\n",
      "follow : follow\n",
      "kesakitan : sakit\n",
      "jeevankaur : jeevankaur\n",
      "dekh : dekh\n",
      "menu : menu\n",
      "eh : eh\n",
      "tweeta : tweeta\n",
      "ala : ala\n",
      "kam : kam\n",
      "jma : jma\n",
      "ni : ni\n",
      "par : par\n",
      "jo : jo\n"
     ]
    }
   ],
   "source": [
    "# import Sastrawi package\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "for document in TWEET_DATA['tweet_normalized']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "            \n",
    "print(len(term_dict))\n",
    "print(\"------------------------\")\n",
    "\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    print(term,\":\" ,term_dict[term])\n",
    "    \n",
    "print(term_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "TWEET_DATA['tweet_tokens_stemmed'] = TWEET_DATA['tweet_normalized'].swifter.apply(get_stemmed_term)\n",
    "print(TWEET_DATA['tweet_tokens_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50491cc9-85b0-478a-97cf-042e28f1edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_DATA.to_csv(\"Text_Preprocessing.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
