{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d80ccf-a352-446e-85b0-cc8e523f13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b294b11c-c1cd-41b7-94a5-30d47be85737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Rating  Category\n",
      "0  Not sure who was more lost - the flat characte...       0  Negative\n",
      "1  Attempting artiness with black & white and cle...       0  Negative\n",
      "2       Very little music or anything to speak of.         0  Negative\n",
      "4  The rest of the movie lacks art, charm, meanin...       0  Negative\n",
      "5                                Wasted two hours.         0  Negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Membaca dataset dari imdb.txt\n",
    "df = pd.read_csv('imdb.txt', delimiter='\\t', header=0, quoting=3)\n",
    "\n",
    "# Memberi nama kolom baru\n",
    "df.columns = ['Review', 'Rating']\n",
    "\n",
    "# Mengkategorikan dataset berdasarkan nilai Rating (0 atau 1)\n",
    "df['Category'] = df['Rating'].apply(lambda x: 'Negative' if x == 0 else 'Positive')\n",
    "\n",
    "# Menambahkan tabel sesuai kategori\n",
    "negative_reviews = df[df['Category'] == 'Negative']\n",
    "positive_reviews = df[df['Category'] == 'Positive']\n",
    "\n",
    "# Menggabungkan kedua tabel menjadi satu\n",
    "combined_df = pd.concat([negative_reviews, positive_reviews])\n",
    "\n",
    "# Menampilkan beberapa baris pertama dari tabel gabungan\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efcbd72a-111c-4644-b337-f3840a94cc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  \n"
     ]
    }
   ],
   "source": [
    "print(df[\"Review\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5f719c5-8c39-402d-b6e2-f668d0bc6f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  \n",
      "Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  \n"
     ]
    }
   ],
   "source": [
    "# Import BeautifulSoup into your workspace\n",
    "from bs4 import BeautifulSoup             \n",
    "\n",
    "# Initialize the BeautifulSoup object on a single movie review     \n",
    "example1 = BeautifulSoup(df[\"Review\"][0])  \n",
    "\n",
    "# Print the raw review and then the output of get_text(), for \n",
    "# comparison\n",
    "print(df[\"Review\"][0])\n",
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33e1400a-4507-48af-a9f4-79943f15a02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not sure who was more lost   the flat characters or the audience  nearly half of whom walked out   \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Use regular expressions to do a find-and-replace\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      example1.get_text() )  # The text to search\n",
    "print(letters_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "471a9b36-a61f-4b13-8556-0c068b7401c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sure', 'lost', 'flat', 'characters', 'audience', 'nearly', 'half', 'walked']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words from \"words\"\n",
    "words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31f221aa-5581-4e5a-a68c-2b10081da0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n",
    "    #text = re.sub(r\"http\", \"\",text)\n",
    "    \n",
    "    html=re.compile(r'<.*?>') \n",
    "    \n",
    "    text = html.sub(r'',text) #Removing html tags\n",
    "    \n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p,'') #Removing punctuations\n",
    "        \n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    \n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    \n",
    "    text = \" \".join(text) #removing stopwords\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) #Removing emojis\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc539037-e0c9-47f1-9cd3-9c99253cb7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sure lost flat character audience, nearly half...</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attempting artiness black white clever camera ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>little music anything speak</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>best scene movie gerardo trying find song keep...</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rest movie lack art, charm, meaning emptiness,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating  Category\n",
       "0  sure lost flat character audience, nearly half...       0  Negative\n",
       "1  attempting artiness black white clever camera ...       0  Negative\n",
       "2                        little music anything speak       0  Negative\n",
       "3  best scene movie gerardo trying find song keep...       1  Positive\n",
       "4  rest movie lack art, charm, meaning emptiness,...       0  Negative"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Review'] = df['Review'].apply(lambda x: clean_text(x))\n",
    " \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301b682-0cd5-49d8-b569-e2fa38e959c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_corpora = df['Review'].iloc[:2].values\n",
    "sample_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e96ec-ad1a-48bb-9346-081eef658b50",
   "metadata": {},
   "source": [
    "Using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35271db0-46ae-41c3-a755-996f1337cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef0d01fb-051e-4b31-8bbd-c349855ca89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c186683b-47db-4570-85e7-b759bffabcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in df[\"Review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c22c4e31-6872-4061-8261-4a44b3c254a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 14:32:19,029 : INFO : collecting all words and their counts\n",
      "2024-07-16 14:32:19,031 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-07-16 14:32:19,034 : INFO : collected 2793 word types from a corpus of 7447 raw words and 997 sentences\n",
      "2024-07-16 14:32:19,035 : INFO : Creating a fresh vocabulary\n",
      "2024-07-16 14:32:19,038 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 12 unique words (0.43% of original 2793, drops 2781)', 'datetime': '2024-07-16T14:32:19.038397', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-07-16 14:32:19,039 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 919 word corpus (12.34% of original 7447, drops 6528)', 'datetime': '2024-07-16T14:32:19.039095', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-07-16 14:32:19,039 : INFO : deleting the raw counts dictionary of 2793 items\n",
      "2024-07-16 14:32:19,039 : INFO : sample=0.001 downsamples 12 most-common words\n",
      "2024-07-16 14:32:19,044 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 106.97885086664279 word corpus (11.6%% of prior 919)', 'datetime': '2024-07-16T14:32:19.044094', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-07-16 14:32:19,044 : INFO : estimated required memory for 12 words and 300 dimensions: 34800 bytes\n",
      "2024-07-16 14:32:19,045 : INFO : resetting layer weights\n",
      "2024-07-16 14:32:19,046 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-07-16T14:32:19.046971', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'build_vocab'}\n",
      "2024-07-16 14:32:19,046 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 12 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2024-07-16T14:32:19.046971', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-07-16 14:32:19,174 : INFO : EPOCH 0: training on 7447 raw words (97 effective words) took 0.0s, 322045 effective words/s\n",
      "2024-07-16 14:32:19,180 : INFO : EPOCH 1: training on 7447 raw words (114 effective words) took 0.0s, 39933 effective words/s\n",
      "2024-07-16 14:32:19,191 : INFO : EPOCH 2: training on 7447 raw words (98 effective words) took 0.0s, 33609 effective words/s\n",
      "2024-07-16 14:32:19,199 : INFO : EPOCH 3: training on 7447 raw words (109 effective words) took 0.0s, 26960 effective words/s\n",
      "2024-07-16 14:32:19,211 : INFO : EPOCH 4: training on 7447 raw words (106 effective words) took 0.0s, 25175 effective words/s\n",
      "2024-07-16 14:32:19,211 : INFO : Word2Vec lifecycle event {'msg': 'training on 37235 raw words (524 effective words) took 0.2s, 3212 effective words/s', 'datetime': '2024-07-16T14:32:19.211353', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-07-16 14:32:19,211 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=12, vector_size=300, alpha=0.025>', 'datetime': '2024-07-16T14:32:19.211353', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n",
      "C:\\Users\\Novia Natasya\\AppData\\Local\\Temp\\ipykernel_19124\\3799526275.py:23: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n",
      "2024-07-16 14:32:19,215 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 14:32:19,215 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-07-16T14:32:19.215865', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'saving'}\n",
      "2024-07-16 14:32:19,215 : INFO : not storing attribute cum_table\n",
      "2024-07-16 14:32:19,215 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            vector_size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6bd118c8-65cb-45b6-aad0-814e8c41b1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 14:32:31,625 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2024-07-16 14:32:31,635 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2024-07-16 14:32:31,641 : INFO : setting ignored attribute cum_table to None\n",
      "2024-07-16 14:32:31,641 : INFO : Word2Vec lifecycle event {'fname': '300features_40minwords_10context', 'datetime': '2024-07-16T14:32:31.641551', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model = Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2588db2a-4c87-4e6b-9481-59735828c671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available words: []\n",
      "Missing words: ['man', 'woman', 'child', 'kitchen']\n"
     ]
    }
   ],
   "source": [
    "words = \"man woman child kitchen\".split()\n",
    "available_words = [word for word in words if word in model.wv]\n",
    "missing_words = [word for word in words if word not in model.wv]\n",
    "\n",
    "print(f\"Available words: {available_words}\")\n",
    "print(f\"Missing words: {missing_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74971098-2f5a-4837-942e-71b5036e43f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      and  document  first  is  one  second  the  third  this\n",
      "Doc0    0         1      1   1    0       0    1      0     1\n",
      "Doc1    0         2      0   1    0       1    1      0     1\n",
      "Doc2    1         0      0   1    1       0    1      1     1\n",
      "Doc3    0         1      1   1    0       0    1      0     1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Your sample corpora\n",
    "sample_corpora = [\"This is the first document.\",\n",
    "                  \"This document is the second document.\",\n",
    "                  \"And this is the third one.\",\n",
    "                  \"Is this the first document?\"]\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "wm = count_vectorizer.fit_transform(sample_corpora)\n",
    "\n",
    "doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n",
    "feat_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "sample_df = pd.DataFrame(data=wm.toarray(), index=doc_names, columns=feat_names)\n",
    "print(sample_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41811a4f-2cc9-4769-880c-41414eb8fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(df['Review'].values,df['Rating'].values,test_size=0.2,random_state=123,stratify=df['Rating'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db66567-f450-43a9-b3b0-713af5a529ac",
   "metadata": {},
   "source": [
    "With new dataset (25,000 IMDB movie reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "19ef354c-f3ed-426a-8b3d-74a1c9133869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from files \n",
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print(\"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
    " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b5019ea-305a-4c58-b7f1-18b4da48de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "72980a97-d17c-4083-98b6-1b0b1ac7eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "40e4b9a9-1efb-4d7b-a33d-eef26684a493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Novia Natasya\\AppData\\Local\\Temp\\ipykernel_19124\\1919973641.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(review).get_text()\n",
      "C:\\Users\\Novia Natasya\\AppData\\Local\\Temp\\ipykernel_19124\\1919973641.py:11: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  review_text = BeautifulSoup(review).get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a3a503eb-8c90-4e76-b337-90bdff9f77a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 15:14:25,616 : INFO : collecting all words and their counts\n",
      "2024-07-16 15:14:25,618 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-07-16 15:14:25,708 : INFO : PROGRESS: at sentence #10000, processed 225664 words, keeping 17775 word types\n",
      "2024-07-16 15:14:25,772 : INFO : PROGRESS: at sentence #20000, processed 451738 words, keeping 24945 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 15:14:25,855 : INFO : PROGRESS: at sentence #30000, processed 670859 words, keeping 30027 word types\n",
      "2024-07-16 15:14:25,939 : INFO : PROGRESS: at sentence #40000, processed 896841 words, keeping 34335 word types\n",
      "2024-07-16 15:14:25,990 : INFO : PROGRESS: at sentence #50000, processed 1116082 words, keeping 37751 word types\n",
      "2024-07-16 15:14:26,078 : INFO : PROGRESS: at sentence #60000, processed 1337544 words, keeping 40711 word types\n",
      "2024-07-16 15:14:26,154 : INFO : PROGRESS: at sentence #70000, processed 1560307 words, keeping 43311 word types\n",
      "2024-07-16 15:14:26,228 : INFO : PROGRESS: at sentence #80000, processed 1779516 words, keeping 45707 word types\n",
      "2024-07-16 15:14:26,298 : INFO : PROGRESS: at sentence #90000, processed 2003714 words, keeping 48121 word types\n",
      "2024-07-16 15:14:26,366 : INFO : PROGRESS: at sentence #100000, processed 2225465 words, keeping 50190 word types\n",
      "2024-07-16 15:14:26,446 : INFO : PROGRESS: at sentence #110000, processed 2444323 words, keeping 52058 word types\n",
      "2024-07-16 15:14:26,502 : INFO : PROGRESS: at sentence #120000, processed 2666488 words, keeping 54098 word types\n",
      "2024-07-16 15:14:26,571 : INFO : PROGRESS: at sentence #130000, processed 2892315 words, keeping 55837 word types\n",
      "2024-07-16 15:14:26,628 : INFO : PROGRESS: at sentence #140000, processed 3104796 words, keeping 57324 word types\n",
      "2024-07-16 15:14:26,682 : INFO : PROGRESS: at sentence #150000, processed 3330432 words, keeping 59045 word types\n",
      "2024-07-16 15:14:26,735 : INFO : PROGRESS: at sentence #160000, processed 3552466 words, keeping 60581 word types\n",
      "2024-07-16 15:14:26,790 : INFO : PROGRESS: at sentence #170000, processed 3776048 words, keeping 62050 word types\n",
      "2024-07-16 15:14:26,844 : INFO : PROGRESS: at sentence #180000, processed 3996237 words, keeping 63483 word types\n",
      "2024-07-16 15:14:26,900 : INFO : PROGRESS: at sentence #190000, processed 4221288 words, keeping 64775 word types\n",
      "2024-07-16 15:14:26,954 : INFO : PROGRESS: at sentence #200000, processed 4445973 words, keeping 66070 word types\n",
      "2024-07-16 15:14:27,014 : INFO : PROGRESS: at sentence #210000, processed 4666511 words, keeping 67367 word types\n",
      "2024-07-16 15:14:27,081 : INFO : PROGRESS: at sentence #220000, processed 4892037 words, keeping 68686 word types\n",
      "2024-07-16 15:14:27,144 : INFO : PROGRESS: at sentence #230000, processed 5113881 words, keeping 69935 word types\n",
      "2024-07-16 15:14:27,208 : INFO : PROGRESS: at sentence #240000, processed 5340847 words, keeping 71144 word types\n",
      "2024-07-16 15:14:27,248 : INFO : PROGRESS: at sentence #250000, processed 5555463 words, keeping 72333 word types\n",
      "2024-07-16 15:14:27,319 : INFO : PROGRESS: at sentence #260000, processed 5775304 words, keeping 73466 word types\n",
      "2024-07-16 15:14:27,360 : INFO : PROGRESS: at sentence #270000, processed 5995572 words, keeping 74740 word types\n",
      "2024-07-16 15:14:27,407 : INFO : PROGRESS: at sentence #280000, processed 6220911 words, keeping 76318 word types\n",
      "2024-07-16 15:14:27,476 : INFO : PROGRESS: at sentence #290000, processed 6443523 words, keeping 77787 word types\n",
      "2024-07-16 15:14:27,537 : INFO : PROGRESS: at sentence #300000, processed 6668258 words, keeping 79142 word types\n",
      "2024-07-16 15:14:27,603 : INFO : PROGRESS: at sentence #310000, processed 6892662 words, keeping 80431 word types\n",
      "2024-07-16 15:14:27,700 : INFO : PROGRESS: at sentence #320000, processed 7118969 words, keeping 81794 word types\n",
      "2024-07-16 15:14:27,755 : INFO : PROGRESS: at sentence #330000, processed 7340486 words, keeping 83006 word types\n",
      "2024-07-16 15:14:27,806 : INFO : PROGRESS: at sentence #340000, processed 7569986 words, keeping 84252 word types\n",
      "2024-07-16 15:14:27,860 : INFO : PROGRESS: at sentence #350000, processed 7792927 words, keeping 85407 word types\n",
      "2024-07-16 15:14:27,909 : INFO : PROGRESS: at sentence #360000, processed 8012526 words, keeping 86567 word types\n",
      "2024-07-16 15:14:27,964 : INFO : PROGRESS: at sentence #370000, processed 8239772 words, keeping 87663 word types\n",
      "2024-07-16 15:14:28,030 : INFO : PROGRESS: at sentence #380000, processed 8465827 words, keeping 88849 word types\n",
      "2024-07-16 15:14:28,090 : INFO : PROGRESS: at sentence #390000, processed 8694607 words, keeping 89883 word types\n",
      "2024-07-16 15:14:28,151 : INFO : PROGRESS: at sentence #400000, processed 8917820 words, keeping 90882 word types\n",
      "2024-07-16 15:14:28,211 : INFO : PROGRESS: at sentence #410000, processed 9138504 words, keeping 91859 word types\n",
      "2024-07-16 15:14:28,286 : INFO : PROGRESS: at sentence #420000, processed 9358474 words, keeping 92880 word types\n",
      "2024-07-16 15:14:28,368 : INFO : PROGRESS: at sentence #430000, processed 9586958 words, keeping 93909 word types\n",
      "2024-07-16 15:14:28,425 : INFO : PROGRESS: at sentence #440000, processed 9812576 words, keeping 94853 word types\n",
      "2024-07-16 15:14:28,484 : INFO : PROGRESS: at sentence #450000, processed 10036719 words, keeping 95995 word types\n",
      "2024-07-16 15:14:28,537 : INFO : PROGRESS: at sentence #460000, processed 10269931 words, keeping 97064 word types\n",
      "2024-07-16 15:14:28,593 : INFO : PROGRESS: at sentence #470000, processed 10496262 words, keeping 97885 word types\n",
      "2024-07-16 15:14:28,648 : INFO : PROGRESS: at sentence #480000, processed 10717170 words, keeping 98809 word types\n",
      "2024-07-16 15:14:28,708 : INFO : PROGRESS: at sentence #490000, processed 10943335 words, keeping 99835 word types\n",
      "2024-07-16 15:14:28,776 : INFO : PROGRESS: at sentence #500000, processed 11165141 words, keeping 100726 word types\n",
      "2024-07-16 15:14:28,847 : INFO : PROGRESS: at sentence #510000, processed 11390498 words, keeping 101672 word types\n",
      "2024-07-16 15:14:28,902 : INFO : PROGRESS: at sentence #520000, processed 11613511 words, keeping 102557 word types\n",
      "2024-07-16 15:14:28,979 : INFO : PROGRESS: at sentence #530000, processed 11838774 words, keeping 103374 word types\n",
      "2024-07-16 15:14:29,043 : INFO : PROGRESS: at sentence #540000, processed 12062185 words, keeping 104231 word types\n",
      "2024-07-16 15:14:29,133 : INFO : PROGRESS: at sentence #550000, processed 12286959 words, keeping 105098 word types\n",
      "2024-07-16 15:14:29,200 : INFO : PROGRESS: at sentence #560000, processed 12509034 words, keeping 105971 word types\n",
      "2024-07-16 15:14:29,255 : INFO : PROGRESS: at sentence #570000, processed 12736827 words, keeping 106757 word types\n",
      "2024-07-16 15:14:29,316 : INFO : PROGRESS: at sentence #580000, processed 12958427 words, keeping 107611 word types\n",
      "2024-07-16 15:14:29,368 : INFO : PROGRESS: at sentence #590000, processed 13184325 words, keeping 108468 word types\n",
      "2024-07-16 15:14:29,419 : INFO : PROGRESS: at sentence #600000, processed 13406551 words, keeping 109189 word types\n",
      "2024-07-16 15:14:29,495 : INFO : PROGRESS: at sentence #610000, processed 13628198 words, keeping 110055 word types\n",
      "2024-07-16 15:14:29,564 : INFO : PROGRESS: at sentence #620000, processed 13852588 words, keeping 110805 word types\n",
      "2024-07-16 15:14:29,632 : INFO : PROGRESS: at sentence #630000, processed 14075901 words, keeping 111573 word types\n",
      "2024-07-16 15:14:29,693 : INFO : PROGRESS: at sentence #640000, processed 14298046 words, keeping 112386 word types\n",
      "2024-07-16 15:14:29,751 : INFO : PROGRESS: at sentence #650000, processed 14522874 words, keeping 113151 word types\n",
      "2024-07-16 15:14:29,820 : INFO : PROGRESS: at sentence #660000, processed 14745445 words, keeping 113890 word types\n",
      "2024-07-16 15:14:29,887 : INFO : PROGRESS: at sentence #670000, processed 14970569 words, keeping 114613 word types\n",
      "2024-07-16 15:14:29,959 : INFO : PROGRESS: at sentence #680000, processed 15194625 words, keeping 115331 word types\n",
      "2024-07-16 15:14:30,020 : INFO : PROGRESS: at sentence #690000, processed 15416773 words, keeping 116099 word types\n",
      "2024-07-16 15:14:30,093 : INFO : PROGRESS: at sentence #700000, processed 15645695 words, keeping 116902 word types\n",
      "2024-07-16 15:14:30,160 : INFO : PROGRESS: at sentence #710000, processed 15865815 words, keeping 117541 word types\n",
      "2024-07-16 15:14:30,221 : INFO : PROGRESS: at sentence #720000, processed 16093342 words, keeping 118183 word types\n",
      "2024-07-16 15:14:30,275 : INFO : PROGRESS: at sentence #730000, processed 16316787 words, keeping 118912 word types\n",
      "2024-07-16 15:14:30,330 : INFO : PROGRESS: at sentence #740000, processed 16539147 words, keeping 119618 word types\n",
      "2024-07-16 15:14:30,385 : INFO : PROGRESS: at sentence #750000, processed 16758552 words, keeping 120264 word types\n",
      "2024-07-16 15:14:30,469 : INFO : PROGRESS: at sentence #760000, processed 16977111 words, keeping 120888 word types\n",
      "2024-07-16 15:14:30,530 : INFO : PROGRESS: at sentence #770000, processed 17203259 words, keeping 121656 word types\n",
      "2024-07-16 15:14:30,578 : INFO : PROGRESS: at sentence #780000, processed 17432844 words, keeping 122358 word types\n",
      "2024-07-16 15:14:30,634 : INFO : PROGRESS: at sentence #790000, processed 17660151 words, keeping 123033 word types\n",
      "2024-07-16 15:14:30,670 : INFO : collected 123504 word types from a corpus of 17798270 raw words and 796172 sentences\n",
      "2024-07-16 15:14:30,670 : INFO : Creating a fresh vocabulary\n",
      "2024-07-16 15:14:30,761 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 16490 unique words (13.35% of original 123504, drops 107014)', 'datetime': '2024-07-16T15:14:30.761265', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-07-16 15:14:30,767 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 17239125 word corpus (96.86% of original 17798270, drops 559145)', 'datetime': '2024-07-16T15:14:30.767416', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-07-16 15:14:30,883 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2024-07-16 15:14:30,883 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2024-07-16 15:14:30,888 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12749798.434354488 word corpus (74.0%% of prior 17239125)', 'datetime': '2024-07-16T15:14:30.888718', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
      "2024-07-16 15:14:31,028 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2024-07-16 15:14:31,034 : INFO : resetting layer weights\n",
      "2024-07-16 15:14:31,061 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-07-16T15:14:31.061157', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'build_vocab'}\n",
      "2024-07-16 15:14:31,061 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2024-07-16T15:14:31.061157', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-07-16 15:14:32,078 : INFO : EPOCH 0 - PROGRESS: at 6.09% examples, 776267 words/s, in_qsize 8, out_qsize 0\n",
      "2024-07-16 15:14:33,098 : INFO : EPOCH 0 - PROGRESS: at 12.98% examples, 811159 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:34,117 : INFO : EPOCH 0 - PROGRESS: at 19.65% examples, 817489 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:35,138 : INFO : EPOCH 0 - PROGRESS: at 26.35% examples, 821771 words/s, in_qsize 8, out_qsize 0\n",
      "2024-07-16 15:14:36,130 : INFO : EPOCH 0 - PROGRESS: at 32.97% examples, 824159 words/s, in_qsize 8, out_qsize 0\n",
      "2024-07-16 15:14:37,147 : INFO : EPOCH 0 - PROGRESS: at 39.54% examples, 824333 words/s, in_qsize 6, out_qsize 1\n",
      "2024-07-16 15:14:38,153 : INFO : EPOCH 0 - PROGRESS: at 46.00% examples, 824627 words/s, in_qsize 8, out_qsize 1\n",
      "2024-07-16 15:14:39,154 : INFO : EPOCH 0 - PROGRESS: at 52.94% examples, 831774 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:40,166 : INFO : EPOCH 0 - PROGRESS: at 59.45% examples, 832251 words/s, in_qsize 8, out_qsize 0\n",
      "2024-07-16 15:14:41,167 : INFO : EPOCH 0 - PROGRESS: at 66.06% examples, 833228 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:42,181 : INFO : EPOCH 0 - PROGRESS: at 72.61% examples, 832850 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:43,174 : INFO : EPOCH 0 - PROGRESS: at 79.06% examples, 831933 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:44,175 : INFO : EPOCH 0 - PROGRESS: at 85.63% examples, 832275 words/s, in_qsize 8, out_qsize 0\n",
      "2024-07-16 15:14:45,175 : INFO : EPOCH 0 - PROGRESS: at 91.87% examples, 829958 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:46,190 : INFO : EPOCH 0 - PROGRESS: at 98.39% examples, 829472 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:46,428 : INFO : EPOCH 0: training on 17798270 raw words (12749331 effective words) took 15.4s, 830408 effective words/s\n",
      "2024-07-16 15:14:47,436 : INFO : EPOCH 1 - PROGRESS: at 6.40% examples, 812784 words/s, in_qsize 8, out_qsize 0\n",
      "2024-07-16 15:14:48,443 : INFO : EPOCH 1 - PROGRESS: at 13.08% examples, 822891 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:49,440 : INFO : EPOCH 1 - PROGRESS: at 19.76% examples, 829082 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:50,454 : INFO : EPOCH 1 - PROGRESS: at 26.40% examples, 831875 words/s, in_qsize 8, out_qsize 0\n",
      "2024-07-16 15:14:51,458 : INFO : EPOCH 1 - PROGRESS: at 33.08% examples, 833216 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:52,452 : INFO : EPOCH 1 - PROGRESS: at 39.59% examples, 832033 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:53,473 : INFO : EPOCH 1 - PROGRESS: at 46.11% examples, 831547 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:54,487 : INFO : EPOCH 1 - PROGRESS: at 52.60% examples, 829438 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:55,495 : INFO : EPOCH 1 - PROGRESS: at 59.17% examples, 830594 words/s, in_qsize 7, out_qsize 1\n",
      "2024-07-16 15:14:56,502 : INFO : EPOCH 1 - PROGRESS: at 65.62% examples, 829564 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:57,501 : INFO : EPOCH 1 - PROGRESS: at 72.04% examples, 828668 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:58,509 : INFO : EPOCH 1 - PROGRESS: at 78.32% examples, 826369 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:14:59,500 : INFO : EPOCH 1 - PROGRESS: at 84.67% examples, 825003 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:00,518 : INFO : EPOCH 1 - PROGRESS: at 91.37% examples, 826904 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:01,538 : INFO : EPOCH 1 - PROGRESS: at 98.00% examples, 827076 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:01,811 : INFO : EPOCH 1: training on 17798270 raw words (12749893 effective words) took 15.4s, 828594 effective words/s\n",
      "2024-07-16 15:15:02,817 : INFO : EPOCH 2 - PROGRESS: at 6.40% examples, 812122 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:03,840 : INFO : EPOCH 2 - PROGRESS: at 13.42% examples, 839363 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:04,871 : INFO : EPOCH 2 - PROGRESS: at 20.39% examples, 843987 words/s, in_qsize 6, out_qsize 1\n",
      "2024-07-16 15:15:05,883 : INFO : EPOCH 2 - PROGRESS: at 27.03% examples, 843099 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:06,883 : INFO : EPOCH 2 - PROGRESS: at 33.83% examples, 844917 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:07,892 : INFO : EPOCH 2 - PROGRESS: at 40.30% examples, 842358 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:08,907 : INFO : EPOCH 2 - PROGRESS: at 46.78% examples, 838849 words/s, in_qsize 8, out_qsize 0\n",
      "2024-07-16 15:15:09,908 : INFO : EPOCH 2 - PROGRESS: at 53.11% examples, 834830 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:10,908 : INFO : EPOCH 2 - PROGRESS: at 59.51% examples, 833825 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:11,907 : INFO : EPOCH 2 - PROGRESS: at 66.01% examples, 833167 words/s, in_qsize 7, out_qsize 2\n",
      "2024-07-16 15:15:12,927 : INFO : EPOCH 2 - PROGRESS: at 72.72% examples, 834350 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:13,931 : INFO : EPOCH 2 - PROGRESS: at 79.63% examples, 837371 words/s, in_qsize 8, out_qsize 0\n",
      "2024-07-16 15:15:14,949 : INFO : EPOCH 2 - PROGRESS: at 86.37% examples, 838643 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:15,949 : INFO : EPOCH 2 - PROGRESS: at 92.85% examples, 837878 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:16,952 : INFO : EPOCH 2 - PROGRESS: at 99.38% examples, 837453 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:17,034 : INFO : EPOCH 2: training on 17798270 raw words (12750004 effective words) took 15.2s, 837722 effective words/s\n",
      "2024-07-16 15:15:18,056 : INFO : EPOCH 3 - PROGRESS: at 6.68% examples, 844912 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:19,050 : INFO : EPOCH 3 - PROGRESS: at 13.54% examples, 855192 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:20,049 : INFO : EPOCH 3 - PROGRESS: at 20.39% examples, 857238 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:21,069 : INFO : EPOCH 3 - PROGRESS: at 27.03% examples, 852312 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:22,065 : INFO : EPOCH 3 - PROGRESS: at 33.61% examples, 846999 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:23,078 : INFO : EPOCH 3 - PROGRESS: at 40.30% examples, 847534 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:24,087 : INFO : EPOCH 3 - PROGRESS: at 46.99% examples, 847640 words/s, in_qsize 8, out_qsize 0\n",
      "2024-07-16 15:15:25,089 : INFO : EPOCH 3 - PROGRESS: at 53.67% examples, 847232 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:26,097 : INFO : EPOCH 3 - PROGRESS: at 60.18% examples, 846309 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:27,100 : INFO : EPOCH 3 - PROGRESS: at 66.90% examples, 847212 words/s, in_qsize 8, out_qsize 0\n",
      "2024-07-16 15:15:28,098 : INFO : EPOCH 3 - PROGRESS: at 73.62% examples, 848252 words/s, in_qsize 6, out_qsize 1\n",
      "2024-07-16 15:15:29,106 : INFO : EPOCH 3 - PROGRESS: at 80.15% examples, 846167 words/s, in_qsize 8, out_qsize 0\n",
      "2024-07-16 15:15:30,113 : INFO : EPOCH 3 - PROGRESS: at 86.81% examples, 846403 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:31,123 : INFO : EPOCH 3 - PROGRESS: at 93.74% examples, 848874 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:32,083 : INFO : EPOCH 3: training on 17798270 raw words (12748892 effective words) took 15.0s, 848059 effective words/s\n",
      "2024-07-16 15:15:33,089 : INFO : EPOCH 4 - PROGRESS: at 6.51% examples, 827208 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:34,096 : INFO : EPOCH 4 - PROGRESS: at 13.37% examples, 843337 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:35,099 : INFO : EPOCH 4 - PROGRESS: at 19.99% examples, 839591 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:36,123 : INFO : EPOCH 4 - PROGRESS: at 26.68% examples, 838408 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:37,122 : INFO : EPOCH 4 - PROGRESS: at 33.55% examples, 843319 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:38,134 : INFO : EPOCH 4 - PROGRESS: at 40.18% examples, 842628 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:39,145 : INFO : EPOCH 4 - PROGRESS: at 46.72% examples, 840906 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:40,157 : INFO : EPOCH 4 - PROGRESS: at 53.33% examples, 839834 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:41,159 : INFO : EPOCH 4 - PROGRESS: at 59.73% examples, 838593 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:42,170 : INFO : EPOCH 4 - PROGRESS: at 66.34% examples, 838378 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:43,197 : INFO : EPOCH 4 - PROGRESS: at 73.06% examples, 838329 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:44,196 : INFO : EPOCH 4 - PROGRESS: at 79.63% examples, 837679 words/s, in_qsize 8, out_qsize 0\n",
      "2024-07-16 15:15:45,212 : INFO : EPOCH 4 - PROGRESS: at 86.70% examples, 841609 words/s, in_qsize 7, out_qsize 0\n",
      "2024-07-16 15:15:46,214 : INFO : EPOCH 4 - PROGRESS: at 93.18% examples, 840495 words/s, in_qsize 8, out_qsize 0\n",
      "2024-07-16 15:15:47,226 : INFO : EPOCH 4 - PROGRESS: at 99.89% examples, 841389 words/s, in_qsize 2, out_qsize 1\n",
      "2024-07-16 15:15:47,248 : INFO : EPOCH 4: training on 17798270 raw words (12748559 effective words) took 15.2s, 841186 effective words/s\n",
      "2024-07-16 15:15:47,248 : INFO : Word2Vec lifecycle event {'msg': 'training on 88991350 raw words (63746679 effective words) took 76.2s, 836780 effective words/s', 'datetime': '2024-07-16T15:15:47.248163', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
      "2024-07-16 15:15:47,248 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=16490, vector_size=300, alpha=0.025>', 'datetime': '2024-07-16T15:15:47.248163', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n",
      "C:\\Users\\Novia Natasya\\AppData\\Local\\Temp\\ipykernel_19124\\1223065018.py:23: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n",
      "2024-07-16 15:15:47,260 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
      "2024-07-16 15:15:47,263 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-07-16T15:15:47.263814', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'saving'}\n",
      "2024-07-16 15:15:47,265 : INFO : not storing attribute cum_table\n",
      "2024-07-16 15:15:47,315 : INFO : saved 300features_40minwords_10context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has been saved\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            vector_size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)\n",
    "\n",
    "print(\"model has been saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "23402caa-cdc7-4580-abe7-359fd67b9f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available words: ['man', 'woman', 'child', 'kitchen']\n",
      "Missing words: []\n",
      "The word that doesn't match is: kitchen\n"
     ]
    }
   ],
   "source": [
    "words = \"man woman child kitchen\".split()\n",
    "available_words = [word for word in words if word in model.wv]\n",
    "missing_words = [word for word in words if word not in model.wv]\n",
    "\n",
    "print(f\"Available words: {available_words}\")\n",
    "print(f\"Missing words: {missing_words}\")\n",
    "\n",
    "if available_words:\n",
    "    outlier = model.wv.doesnt_match(available_words)\n",
    "    print(f\"The word that doesn't match is: {outlier}\")\n",
    "else:\n",
    "    print(\"No available words in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e1476-02de-4213-83cc-77032d6370bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
