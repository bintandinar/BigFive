from transformers import BertTokenizer, BertModel
import torch
import numpy as np


# Load IndoBERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained("indobenchmark/indobert-base-p2", force_download=True)
model = BertModel.from_pretrained("indobenchmark/indobert-base-p2", force_download=True)


# Example usage
text = "Halo, apa kabar?"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)

# Get the embeddings
embeddings = outputs.last_hidden_state
print("last hidden state:", embeddings)





# Ekstraksi embedding dari CLS token
cls_embedding = embeddings[:, 0, :].squeeze().detach().numpy()
print("CLS Embedding Shape:", cls_embedding.shape)

# Optionally, visualize the embeddings using libraries like matplotlib or seaborn
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")
plt.figure(figsize=(12, 6))
sns.heatmap(cls_embedding.reshape(1, -1), cmap="viridis", cbar=True)
plt.title("CLS Token Embedding")
plt.show()


# Membandingkan embedding kata yang sama dalam konteks yang berbeda

from transformers import BertTokenizer, BertModel
import torch
from scipy.spatial.distance import cosine

# Load IndoBERT tokenizer and model
# tokenizer = BertTokenizer.from_pretrained("indolem/indobert-base-uncased")
# model = BertModel.from_pretrained("indolem/indobert-base-uncased")

# Sample sentences for exploration
sentences = ["Saya suka makan nasi.", "Nasi adalah makanan pokok di Indonesia."]
embeddings = []
nasi_indices = []

# Get embeddings and indices of the word 'nasi'
for sentence in sentences:
    inputs = tokenizer(sentence, return_tensors="pt")
    outputs = model(**inputs)
    embeddings.append(outputs.last_hidden_state[0].detach().numpy())
    
    # Find index of 'nasi' in the tokenized input
    tokenized_sentence = tokenizer.tokenize(sentence)
    nasi_idx = tokenized_sentence.index("nasi")
    nasi_indices.append(nasi_idx)

# Extract embeddings for 'nasi'
nasi_embedding1 = embeddings[0][nasi_indices[0]]
nasi_embedding2 = embeddings[1][nasi_indices[1]]

# Calculate cosine similarity between the embeddings of the word 'nasi'
similarity_nasi = 1 - cosine(nasi_embedding1, nasi_embedding2)
print(f"Cosine similarity between 'nasi' in different contexts: {similarity_nasi}")


# Visualisasi embedding 

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

all_tokens = []
all_embeddings = []


# Get embeddings and tokens for each sentence
for sentence in sentences:
    inputs = tokenizer(sentence, return_tensors="pt")
    outputs = model(**inputs)
    embeddings = outputs.last_hidden_state[0].detach().numpy()
    
    tokenized_sentence = tokenizer.tokenize(sentence)
    all_tokens.extend(tokenized_sentence)
    all_embeddings.extend(embeddings)

# Convert list to numpy array
all_embeddings = np.array(all_embeddings)

# Perform PCA on the embeddings
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(all_embeddings)

# Plot the reduced embeddings
plt.figure(figsize=(10, 10))
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])

for i, token in enumerate(all_tokens):
    plt.annotate(token, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]))

plt.title("PCA of Token Embeddings")
plt.xlabel("PCA1")
plt.ylabel("PCA2")
plt.show()




